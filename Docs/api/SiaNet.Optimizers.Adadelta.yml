### YamlMime:ManagedReference
items:
- uid: SiaNet.Optimizers.Adadelta
  commentId: T:SiaNet.Optimizers.Adadelta
  id: Adadelta
  parent: SiaNet.Optimizers
  children:
  - SiaNet.Optimizers.Adadelta.#ctor(System.Single,System.Single,System.Single,System.Single)
  - SiaNet.Optimizers.Adadelta.Epsilon
  - SiaNet.Optimizers.Adadelta.Rho
  langs:
  - csharp
  - vb
  name: Adadelta
  nameWithType: Adadelta
  fullName: SiaNet.Optimizers.Adadelta
  type: Class
  source:
    remote:
      path: SiaNet/Optimizers/Adadelta.cs
      branch: master
      repo: https://github.com/deepakkumar1984/SiaNet
    id: Adadelta
    path: ../SiaNet/Optimizers/Adadelta.cs
    startLine: 10
  assemblies:
  - SiaNet
  namespace: SiaNet.Optimizers
  summary: "\nAdadelta is a more robust extension of Adagrad that adapts learning rates based on a moving window of gradient updates, instead of accumulating all past gradients. This way, Adadelta continues learning even when many updates have been done. Compared to Adagrad, in the original version of Adadelta you don&apos;t have to set an initial learning rate. In this version, initial learning rate and decay factor can be set, as in most other optimizers.\n"
  example: []
  syntax:
    content: 'public class Adadelta : BaseOptimizer'
    content.vb: >-
      Public Class Adadelta
          Inherits BaseOptimizer
  seealso:
  - linkId: SiaNet.Optimizers.BaseOptimizer
    commentId: T:SiaNet.Optimizers.BaseOptimizer
  inheritance:
  - System.Object
  - SiaNet.Optimizers.BaseOptimizer
  inheritedMembers:
  - SiaNet.Optimizers.BaseOptimizer.Name
  - SiaNet.Optimizers.BaseOptimizer.LearningRate
  - SiaNet.Optimizers.BaseOptimizer.Momentum
  - SiaNet.Optimizers.BaseOptimizer.DecayRate
  modifiers.csharp:
  - public
  - class
  modifiers.vb:
  - Public
  - Class
- uid: SiaNet.Optimizers.Adadelta.Rho
  commentId: P:SiaNet.Optimizers.Adadelta.Rho
  id: Rho
  parent: SiaNet.Optimizers.Adadelta
  langs:
  - csharp
  - vb
  name: Rho
  nameWithType: Adadelta.Rho
  fullName: SiaNet.Optimizers.Adadelta.Rho
  type: Property
  source:
    remote:
      path: SiaNet/Optimizers/Adadelta.cs
      branch: master
      repo: https://github.com/deepakkumar1984/SiaNet
    id: Rho
    path: ../SiaNet/Optimizers/Adadelta.cs
    startLine: 18
  assemblies:
  - SiaNet
  namespace: SiaNet.Optimizers
  summary: "\nAdadelta decay factor, corresponding to fraction of gradient to keep at each time step.\n"
  example: []
  syntax:
    content: public float Rho { get; set; }
    parameters: []
    return:
      type: System.Single
      description: "\nThe rho.\n"
    content.vb: Public Property Rho As Single
  overload: SiaNet.Optimizers.Adadelta.Rho*
  modifiers.csharp:
  - public
  - get
  - set
  modifiers.vb:
  - Public
- uid: SiaNet.Optimizers.Adadelta.Epsilon
  commentId: P:SiaNet.Optimizers.Adadelta.Epsilon
  id: Epsilon
  parent: SiaNet.Optimizers.Adadelta
  langs:
  - csharp
  - vb
  name: Epsilon
  nameWithType: Adadelta.Epsilon
  fullName: SiaNet.Optimizers.Adadelta.Epsilon
  type: Property
  source:
    remote:
      path: SiaNet/Optimizers/Adadelta.cs
      branch: master
      repo: https://github.com/deepakkumar1984/SiaNet
    id: Epsilon
    path: ../SiaNet/Optimizers/Adadelta.cs
    startLine: 26
  assemblies:
  - SiaNet
  namespace: SiaNet.Optimizers
  summary: "\nFuzz factor. Lowest float value but > 0\n"
  example: []
  syntax:
    content: public float Epsilon { get; set; }
    parameters: []
    return:
      type: System.Single
      description: "\nThe epsilon.\n"
    content.vb: Public Property Epsilon As Single
  overload: SiaNet.Optimizers.Adadelta.Epsilon*
  modifiers.csharp:
  - public
  - get
  - set
  modifiers.vb:
  - Public
- uid: SiaNet.Optimizers.Adadelta.#ctor(System.Single,System.Single,System.Single,System.Single)
  commentId: M:SiaNet.Optimizers.Adadelta.#ctor(System.Single,System.Single,System.Single,System.Single)
  id: '#ctor(System.Single,System.Single,System.Single,System.Single)'
  parent: SiaNet.Optimizers.Adadelta
  langs:
  - csharp
  - vb
  name: Adadelta(Single, Single, Single, Single)
  nameWithType: Adadelta.Adadelta(Single, Single, Single, Single)
  fullName: SiaNet.Optimizers.Adadelta.Adadelta(System.Single, System.Single, System.Single, System.Single)
  type: Constructor
  source:
    remote:
      path: SiaNet/Optimizers/Adadelta.cs
      branch: master
      repo: https://github.com/deepakkumar1984/SiaNet
    id: .ctor
    path: ../SiaNet/Optimizers/Adadelta.cs
    startLine: 39
  assemblies:
  - SiaNet
  namespace: SiaNet.Optimizers
  summary: "\nInitializes a new instance of the <xref href=\"SiaNet.Optimizers.Adadelta\" data-throw-if-not-resolved=\"false\"></xref> class.\n"
  example: []
  syntax:
    content: public Adadelta(float lr = 1F, float rho = 0.95F, float decayRate = 0F, float epsilon = 1E-07F)
    parameters:
    - id: lr
      type: System.Single
      description: Initial learning rate, defaults to 1. It is recommended to leave it at the default value.
    - id: rho
      type: System.Single
      description: Adadelta decay factor, corresponding to fraction of gradient to keep at each time step.
    - id: decayRate
      type: System.Single
      description: Learning rate decay factor over each update.
    - id: epsilon
      type: System.Single
      description: The epsilon.
    content.vb: Public Sub New(lr As Single = 1F, rho As Single = 0.95F, decayRate As Single = 0F, epsilon As Single = 1E-07F)
  overload: SiaNet.Optimizers.Adadelta.#ctor*
  modifiers.csharp:
  - public
  modifiers.vb:
  - Public
references:
- uid: SiaNet.Optimizers.BaseOptimizer
  commentId: T:SiaNet.Optimizers.BaseOptimizer
  parent: SiaNet.Optimizers
  name: BaseOptimizer
  nameWithType: BaseOptimizer
  fullName: SiaNet.Optimizers.BaseOptimizer
- uid: SiaNet.Optimizers
  commentId: N:SiaNet.Optimizers
  name: SiaNet.Optimizers
  nameWithType: SiaNet.Optimizers
  fullName: SiaNet.Optimizers
- uid: System.Object
  commentId: T:System.Object
  parent: System
  isExternal: true
  name: Object
  nameWithType: Object
  fullName: System.Object
- uid: SiaNet.Optimizers.BaseOptimizer.Name
  commentId: P:SiaNet.Optimizers.BaseOptimizer.Name
  parent: SiaNet.Optimizers.BaseOptimizer
  name: Name
  nameWithType: BaseOptimizer.Name
  fullName: SiaNet.Optimizers.BaseOptimizer.Name
- uid: SiaNet.Optimizers.BaseOptimizer.LearningRate
  commentId: P:SiaNet.Optimizers.BaseOptimizer.LearningRate
  parent: SiaNet.Optimizers.BaseOptimizer
  name: LearningRate
  nameWithType: BaseOptimizer.LearningRate
  fullName: SiaNet.Optimizers.BaseOptimizer.LearningRate
- uid: SiaNet.Optimizers.BaseOptimizer.Momentum
  commentId: P:SiaNet.Optimizers.BaseOptimizer.Momentum
  parent: SiaNet.Optimizers.BaseOptimizer
  name: Momentum
  nameWithType: BaseOptimizer.Momentum
  fullName: SiaNet.Optimizers.BaseOptimizer.Momentum
- uid: SiaNet.Optimizers.BaseOptimizer.DecayRate
  commentId: P:SiaNet.Optimizers.BaseOptimizer.DecayRate
  parent: SiaNet.Optimizers.BaseOptimizer
  name: DecayRate
  nameWithType: BaseOptimizer.DecayRate
  fullName: SiaNet.Optimizers.BaseOptimizer.DecayRate
- uid: System
  commentId: N:System
  isExternal: true
  name: System
  nameWithType: System
  fullName: System
- uid: SiaNet.Optimizers.Adadelta.Rho*
  commentId: Overload:SiaNet.Optimizers.Adadelta.Rho
  name: Rho
  nameWithType: Adadelta.Rho
  fullName: SiaNet.Optimizers.Adadelta.Rho
- uid: System.Single
  commentId: T:System.Single
  parent: System
  isExternal: true
  name: Single
  nameWithType: Single
  fullName: System.Single
- uid: SiaNet.Optimizers.Adadelta.Epsilon*
  commentId: Overload:SiaNet.Optimizers.Adadelta.Epsilon
  name: Epsilon
  nameWithType: Adadelta.Epsilon
  fullName: SiaNet.Optimizers.Adadelta.Epsilon
- uid: SiaNet.Optimizers.Adadelta
  commentId: T:SiaNet.Optimizers.Adadelta
  name: Adadelta
  nameWithType: Adadelta
  fullName: SiaNet.Optimizers.Adadelta
- uid: SiaNet.Optimizers.Adadelta.#ctor*
  commentId: Overload:SiaNet.Optimizers.Adadelta.#ctor
  name: Adadelta
  nameWithType: Adadelta.Adadelta
  fullName: SiaNet.Optimizers.Adadelta.Adadelta
