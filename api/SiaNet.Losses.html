<!DOCTYPE html>
<!--[if IE]><![endif]-->
<html>
  
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>Namespace SiaNet.Losses
   </title>
    <meta name="viewport" content="width=device-width">
    <meta name="title" content="Namespace SiaNet.Losses
   ">
    <meta name="generator" content="docfx 2.40.12.0">
    
    <link rel="shortcut icon" href="../favicon.ico">
    <link rel="stylesheet" href="../styles/docfx.vendor.css">
    <link rel="stylesheet" href="../styles/docfx.css">
    <link rel="stylesheet" href="../styles/main.css">
    <link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet"> 
    <meta property="docfx:navrel" content="../toc.html">
    <meta property="docfx:tocrel" content="toc.html">
    
    
    
  </head>  <body data-spy="scroll" data-target="#affix" data-offset="120">
    <div id="wrapper">
      <header>
        
        <nav id="autocollapse" class="navbar navbar-inverse ng-scope" role="navigation">
          <div class="container">
            <div class="navbar-header">
              <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#navbar">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
              </button>
              
              <a class="navbar-brand" href="../index.html">
                <img id="logo" class="svg" src="../logo.svg" alt="">
              </a>
            </div>
            <div class="collapse navbar-collapse" id="navbar">
              <form class="navbar-form navbar-right" role="search" id="search">
                <div class="form-group">
                  <input type="text" class="form-control" id="search-query" placeholder="Search" autocomplete="off">
                </div>
              </form>
            </div>
          </div>
        </nav>
        
        <div class="subnav navbar navbar-default">
          <div class="container hide-when-search" id="breadcrumb">
            <ul class="breadcrumb">
              <li></li>
            </ul>
          </div>
        </div>
      </header>
      <div role="main" class="container body-content hide-when-search">
        
        <div class="sidenav hide-when-search">
          <a class="btn toc-toggle collapse" data-toggle="collapse" href="#sidetoggle" aria-expanded="false" aria-controls="sidetoggle">Show / Hide Table of Contents</a>
          <div class="sidetoggle collapse" id="sidetoggle">
            <div id="sidetoc"></div>
          </div>
        </div>
        <div class="article row grid-right">
          <div class="col-md-10">
            <article class="content wrap" id="_content" data-uid="SiaNet.Losses">
  
  <h1 id="SiaNet_Losses" data-uid="SiaNet.Losses" class="text-break">Namespace SiaNet.Losses
  </h1>
  <div class="markdown level0 summary"></div>
  <div class="markdown level0 conceptual"></div>
  <div class="markdown level0 remarks"></div>
    <h3 id="classes">Classes
  </h3>
      <h4><a class="xref" href="SiaNet.Losses.BaseLoss.html">BaseLoss</a></h4>
      <section><p>A loss function (or objective function, or optimization score function) is one of the two parameters required to compile a model.</p>
</section>
      <h4><a class="xref" href="SiaNet.Losses.BinaryCrossentropy.html">BinaryCrossentropy</a></h4>
      <section><p>Binary Cross-Entropy Loss. Also called Sigmoid Cross-Entropy loss.
It is a Sigmoid activation plus a Cross-Entropy loss.
Unlike Softmax loss it is independent for each vector component (class), meaning that the loss computed for every vector component is not affected by other component values</p>
</section>
      <h4><a class="xref" href="SiaNet.Losses.CategorialHinge.html">CategorialHinge</a></h4>
      <section><p>The categorical cross-entropy loss (negative log likelihood) is used when a probabilistic interpretation of the scores is desired.</p>
</section>
      <h4><a class="xref" href="SiaNet.Losses.CategoricalCrossentropy.html">CategoricalCrossentropy</a></h4>
      <section><p>Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1.
Cross-entropy loss increases as the predicted probability diverges from the actual label.</p>
</section>
      <h4><a class="xref" href="SiaNet.Losses.CosineProximity.html">CosineProximity</a></h4>
      <section><p>Cosine Proximity loss function computes the cosine proximity between predicted value and actual value.
It is same as Cosine Similarity, which is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them. In this case, note that unit vectors are maximally “similar” if they’re parallel and maximally “dissimilar” if they’re orthogonal (perpendicular).
This is analogous to the cosine, which is unity (maximum value) when the segments subtend a zero angle and zero (uncorrelated) when the segments are perpendicular.</p>
</section>
      <h4><a class="xref" href="SiaNet.Losses.Hinge.html">Hinge</a></h4>
      <section><p>Hinge Loss, also known as max-margin objective, is a loss function used for training classifiers.
The hinge loss is used for “maximum-margin” classification, most notably for support vector machines (SVMs).</p>
</section>
      <h4><a class="xref" href="SiaNet.Losses.KullbackLeiblerDivergence.html">KullbackLeiblerDivergence</a></h4>
      <section><p>KL Divergence, also known as relative entropy, information divergence/gain, is a measure of how one probability distribution diverges from a second expected probability distribution.</p>
</section>
      <h4><a class="xref" href="SiaNet.Losses.LogCosh.html">LogCosh</a></h4>
      <section><p>Logarithm of the hyperbolic cosine of the prediction error.</p>
<p>
log(cosh(x)) is approximately equal to(x** 2) / 2 for small x and to abs(x) - log(2) for large x.
This means that &apos;logcosh&apos; works mostly like the mean squared error, but will not be so strongly affected by the occasional wildly incorrect prediction.
</p>
</section>
      <h4><a class="xref" href="SiaNet.Losses.MeanAbsoluteError.html">MeanAbsoluteError</a></h4>
      <section><p>Mean Absolute Error (MAE) is a quantity used to measure how close forecasts or predictions are to the eventual outcomes</p>
</section>
      <h4><a class="xref" href="SiaNet.Losses.MeanAbsolutePercentageError.html">MeanAbsolutePercentageError</a></h4>
      <section><p>The mean absolute percentage error (MAPE), also known as mean absolute percentage deviation (MAPD),
is a measure of prediction accuracy of a forecasting method in statistics, for example in trend estimation, also used as a Loss function for regression problems in Machine Learning.</p>
</section>
      <h4><a class="xref" href="SiaNet.Losses.MeanSquaredError.html">MeanSquaredError</a></h4>
      <section><p>The mean squared error (MSE) or mean squared deviation (MSD) of an estimator (of a procedure for estimating an unobserved quantity) measures the average of the squares of the errors—that is, the average squared difference between the estimated values and what is estimated.</p>
</section>
      <h4><a class="xref" href="SiaNet.Losses.MeanSquaredLogError.html">MeanSquaredLogError</a></h4>
      <section><p>Mean squared logarithmic error (MSLE) is, as the name suggests, a variation of the Mean Squared Error.
The loss is the mean over the seen data of the squared differences between the log-transformed true and predicted values, or writing it as a formula: where ŷ is the predicted value.</p>
</section>
      <h4><a class="xref" href="SiaNet.Losses.Poisson.html">Poisson</a></h4>
      <section><p>Poisson loss function is a measure of how the predicted distribution diverges from the expected distribution,
the poisson as loss function is a variant from Poisson Distribution, where the poisson distribution is widely used for modeling count data.
It can be shown to be the limiting distribution for a normal approximation to a binomial where the number of trials goes to infinity and the probability
goes to zero and both happen at such a rate that np is equal to some mean frequency for the process.</p>
</section>
      <h4><a class="xref" href="SiaNet.Losses.SquaredHinge.html">SquaredHinge</a></h4>
      <section><p>Squared Hinge Loss function is a variant of Hinge Loss, it solves the problem in hinge loss that the derivative of hinge loss has a discontinuity at Pred * True = 1</p>
</section>
</article>
          </div>
          
          <div class="hidden-sm col-md-2" role="complementary">
            <div class="sideaffix">
              <div class="contribution">
                <ul class="nav">
                </ul>
              </div>
              <nav class="bs-docs-sidebar hidden-print hidden-xs hidden-sm affix" id="affix">
              <!-- <p><a class="back-to-top" href="#top">Back to top</a><p> -->
              </nav>
            </div>
          </div>
        </div>
      </div>
      
      <footer>
        <div class="grad-bottom"></div>
        <div class="footer">
          <div class="container">
            <span class="pull-right">
              <a href="#top">Back to top</a>
            </span>
            
            <span>Generated by <strong>DocFX</strong></span>
          </div>
        </div>
      </footer>
    </div>
    
    <script type="text/javascript" src="../styles/docfx.vendor.js"></script>
    <script type="text/javascript" src="../styles/docfx.js"></script>
    <script type="text/javascript" src="../styles/main.js"></script>
  </body>
</html>
