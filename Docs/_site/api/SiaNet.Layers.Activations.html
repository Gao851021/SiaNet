<!DOCTYPE html>
<!--[if IE]><![endif]-->
<html>
  
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>Namespace SiaNet.Layers.Activations
   </title>
    <meta name="viewport" content="width=device-width">
    <meta name="title" content="Namespace SiaNet.Layers.Activations
   ">
    <meta name="generator" content="docfx 2.40.12.0">
    
    <link rel="shortcut icon" href="../favicon.ico">
    <link rel="stylesheet" href="../styles/docfx.vendor.css">
    <link rel="stylesheet" href="../styles/docfx.css">
    <link rel="stylesheet" href="../styles/main.css">
    <link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet"> 
    <meta property="docfx:navrel" content="../toc.html">
    <meta property="docfx:tocrel" content="toc.html">
    
    
    
  </head>  <body data-spy="scroll" data-target="#affix" data-offset="120">
    <div id="wrapper">
      <header>
        
        <nav id="autocollapse" class="navbar navbar-inverse ng-scope" role="navigation">
          <div class="container">
            <div class="navbar-header">
              <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#navbar">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
              </button>
              
              <a class="navbar-brand" href="../index.html">
                <img id="logo" class="svg" src="../logo.svg" alt="">
              </a>
            </div>
            <div class="collapse navbar-collapse" id="navbar">
              <form class="navbar-form navbar-right" role="search" id="search">
                <div class="form-group">
                  <input type="text" class="form-control" id="search-query" placeholder="Search" autocomplete="off">
                </div>
              </form>
            </div>
          </div>
        </nav>
        
        <div class="subnav navbar navbar-default">
          <div class="container hide-when-search" id="breadcrumb">
            <ul class="breadcrumb">
              <li></li>
            </ul>
          </div>
        </div>
      </header>
      <div role="main" class="container body-content hide-when-search">
        
        <div class="sidenav hide-when-search">
          <a class="btn toc-toggle collapse" data-toggle="collapse" href="#sidetoggle" aria-expanded="false" aria-controls="sidetoggle">Show / Hide Table of Contents</a>
          <div class="sidetoggle collapse" id="sidetoggle">
            <div id="sidetoc"></div>
          </div>
        </div>
        <div class="article row grid-right">
          <div class="col-md-10">
            <article class="content wrap" id="_content" data-uid="SiaNet.Layers.Activations">
  
  <h1 id="SiaNet_Layers_Activations" data-uid="SiaNet.Layers.Activations" class="text-break">Namespace SiaNet.Layers.Activations
  </h1>
  <div class="markdown level0 summary"></div>
  <div class="markdown level0 conceptual"></div>
  <div class="markdown level0 remarks"></div>
    <h3 id="classes">Classes
  </h3>
      <h4><a class="xref" href="SiaNet.Layers.Activations.Elu.html">Elu</a></h4>
      <section><p>Exponential linear unit activation function: x if x &gt; 0 and alpha * (exp(x)-1) if x &lt; 0.</p>
</section>
      <h4><a class="xref" href="SiaNet.Layers.Activations.Exp.html">Exp</a></h4>
      <section><p>Exponential activation function which returns simple exp(x)</p>
</section>
      <h4><a class="xref" href="SiaNet.Layers.Activations.HardSigmoid.html">HardSigmoid</a></h4>
      <section></section>
      <h4><a class="xref" href="SiaNet.Layers.Activations.LeakyRelu.html">LeakyRelu</a></h4>
      <section><p>Leaky version of a Rectified Linear Unit.</p>
<p>
It allows a small gradient when the unit is not active: f(x) = alpha* x for x&lt; 0, f(x) = x for x >= 0.
</p>
</section>
      <h4><a class="xref" href="SiaNet.Layers.Activations.Linear.html">Linear</a></h4>
      <section><p>Linear activation with f(x)=x</p>
</section>
      <h4><a class="xref" href="SiaNet.Layers.Activations.PRelu.html">PRelu</a></h4>
      <section><p>Parametric Rectified Linear Unit.</p>
<p>
It follows: f(x) = alpha* x for x&lt; 0, f(x) = x for x >= 0, where alpha is a learned array with the same shape as x.
</p>
</section>
      <h4><a class="xref" href="SiaNet.Layers.Activations.Relu.html">Relu</a></h4>
      <section><p>Rectified Linear Unit.</p>
<p>
With default values, it returns element-wise max(x, 0).
</p>
<p>
Otherwise, it follows: f(x) = max_value for x >= max_value, f(x) = x for threshold &lt;= x&lt;max_value, f(x) = alpha* (x - threshold) otherwise.
</p>
</section>
      <h4><a class="xref" href="SiaNet.Layers.Activations.Selu.html">Selu</a></h4>
      <section><p>SELU is equal to: scale * elu(x, alpha), where alpha and scale are predefined constants.
The values of alpha and scale are chosen so that the mean and variance of the inputs are preserved between two consecutive layers as long as the weights are initialized correctly (see lecun_normal initialization)
and the number of inputs is &quot;large enough&quot;</p>
</section>
      <h4><a class="xref" href="SiaNet.Layers.Activations.Sigmoid.html">Sigmoid</a></h4>
      <section><p>Sigmoid takes a real value as input and outputs another value between 0 and 1.
It’s easy to work with and has all the nice properties of activation functions: it’s non-linear, continuously differentiable, monotonic, and has a fixed output range.</p>
</section>
      <h4><a class="xref" href="SiaNet.Layers.Activations.Softmax.html">Softmax</a></h4>
      <section><p>Softmax function calculates the probabilities distribution of the event over ‘n’ different events.
In general way of saying, this function will calculate the probabilities of each target class over all possible target classes.
Later the calculated probabilities will be helpful for determining the target class for the given inputs.</p>
</section>
      <h4><a class="xref" href="SiaNet.Layers.Activations.Softplus.html">Softplus</a></h4>
      <section><p>The softplus activation: log(exp(x) + 1).</p>
</section>
      <h4><a class="xref" href="SiaNet.Layers.Activations.Softsign.html">Softsign</a></h4>
      <section><p>The softsign activation: x / (abs(x) + 1).</p>
</section>
      <h4><a class="xref" href="SiaNet.Layers.Activations.Tanh.html">Tanh</a></h4>
      <section><p>Hyperbolic tangent activation. Tanh squashes a real-valued number to the range [-1, 1].
It’s non-linear. But unlike Sigmoid, its output is zero-centered. Therefore, in practice the tanh non-linearity is always preferred to the sigmoid nonlinearity.</p>
</section>
</article>
          </div>
          
          <div class="hidden-sm col-md-2" role="complementary">
            <div class="sideaffix">
              <div class="contribution">
                <ul class="nav">
                </ul>
              </div>
              <nav class="bs-docs-sidebar hidden-print hidden-xs hidden-sm affix" id="affix">
              <!-- <p><a class="back-to-top" href="#top">Back to top</a><p> -->
              </nav>
            </div>
          </div>
        </div>
      </div>
      
      <footer>
        <div class="grad-bottom"></div>
        <div class="footer">
          <div class="container">
            <span class="pull-right">
              <a href="#top">Back to top</a>
            </span>
            
            <span>Generated by <strong>DocFX</strong></span>
          </div>
        </div>
      </footer>
    </div>
    
    <script type="text/javascript" src="../styles/docfx.vendor.js"></script>
    <script type="text/javascript" src="../styles/docfx.js"></script>
    <script type="text/javascript" src="../styles/main.js"></script>
  </body>
</html>
